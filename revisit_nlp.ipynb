{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-12 18:09:42.130936: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "# import packages\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['title', 'total_game_hours_last_two_weeks', 'num_groups', 'orig_url',\n",
      "       'num_badges', 'review_url', 'num_found_funny', 'review', 'date_updated',\n",
      "       'num_workshop_items', 'date_posted', 'found_helpful_percentage',\n",
      "       'num_voted_helpfulness', 'achievement_progress', 'profile_url',\n",
      "       'num_found_helpful', 'steam_id_number', 'friend_player_level',\n",
      "       'num_found_unhelpful', 'total_game_hours', 'username', 'num_guides',\n",
      "       'rating', 'num_friends', 'num_screenshots', 'num_comments',\n",
      "       'num_reviews', 'num_games_owned'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# target column name is \"found_helpful_percentage\"\n",
    "input_files = glob.glob('data/*')\n",
    "dfs = []\n",
    "for f in input_files:\n",
    "    df = pd.read_json(f, lines=True)\n",
    "    df.insert(0, 'title', Path(f).stem)\n",
    "    dfs.append(df)\n",
    "df_combined = pd.concat(dfs)\n",
    "print(df_combined.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79437\n",
      "9482\n",
      "6279\n",
      "5184\n"
     ]
    }
   ],
   "source": [
    "# select only records that have significant amount of usefulness votes\n",
    "print(len(df_combined.num_voted_helpfulness))\n",
    "print(len(df_combined[df_combined.num_voted_helpfulness>10]))\n",
    "print(len(df_combined[df_combined.num_voted_helpfulness>20]))\n",
    "print(len(df_combined[df_combined.num_voted_helpfulness>30]))\n",
    "# for now, choose 10 as the selection rule\n",
    "df_sel = df_combined[df_combined.num_voted_helpfulness>10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_review = df_sel[['review']]\n",
    "df_review = df_sel['review']\n",
    "df_target = df_sel['found_helpful_percentage']\n",
    "X_train, X_test , y_train, y_test = train_test_split(df_review, df_target, test_size = 0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some (hyper)parameter settings\n",
    "# ref: https://heartbeat.comet.ml/text-classification-using-long-short-term-memory-glove-embeddings-6894abb730e1\n",
    "vocab_size = 1000\n",
    "oov_token = '<OOV>'\n",
    "max_length = 100\n",
    "padding_type = 'post'\n",
    "truncation_type = 'post'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_token)\n",
    "tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "X_train_padded = pad_sequences(X_train_sequences, maxlen=max_length, padding=padding_type, truncating=truncation_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "X_test_padded = pad_sequences(X_test_sequences, maxlen=max_length, padding=padding_type, truncating=truncation_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the pretrained GloVe pre-trained word vectors if not exists\n",
    "# $ conda install -c conda-forge python-wget\n",
    "import wget\n",
    "model_url = 'https://nlp.stanford.edu/data/glove.6B.zip'\n",
    "out_dir = 'model/glove'\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "out_fpn = os.path.join(out_dir, 'glove.6B.zip')\n",
    "if not os.path.exists(out_fpn):\n",
    "    model_file = wget.download(model_url, out=out_fpn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['glove.6B.50d.txt', 'glove.6B.100d.txt', 'glove.6B.200d.txt', 'glove.6B.300d.txt']\n"
     ]
    }
   ],
   "source": [
    "# obtain the token for the zip file without unzip\n",
    "import zipfile\n",
    "unzipped_model = zipfile.ZipFile(out_fpn, 'r')\n",
    "print(unzipped_model.namelist())\n",
    "\n",
    "# extract the single file for use\n",
    "# since extracting on the fly takes much much time...\n",
    "ext_file = f'glove.6B.{max_length}d.txt'\n",
    "ext_model_fpn = os.path.join(out_dir, ext_file)\n",
    "if not os.path.exists(ext_model_fpn):\n",
    "    unzipped_model.extract(ext_file, out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# load the pretrained vector\n",
    "embeddings_index = {}\n",
    "f = open(ext_model_fpn)\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different Models\n",
    "Up to the previous cell, the training and test datasets are vectorized and ready to be input to a model.  \n",
    "I'll attempt to try out the XGBoost model first.  \n",
    "If time is allowed, I will play with LSTM.  \n",
    "\n",
    "## XGBoost\n",
    "A seemingly very nice article is found [here](https://edumunozsala.github.io/BlogEms/jupyter/nlp/classification/embeddings/python/2020/08/15/Intro_NLP_WordEmbeddings_Classification.html).  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, max_length))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Use third party utilities.\n",
    "Ref:\n",
    "https://edumunozsala.github.io/BlogEms/jupyter/nlp/classification/embeddings/python/2020/08/15/Intro_NLP_WordEmbeddings_Classification.html\n",
    "'''\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from pathlib import Path\n",
    "\n",
    "word2vec_output_fpn = os.path.join('model/glove', Path(ext_model_fpn).stem + '.w2v')\n",
    "if not os.path.exists(word2vec_output_fpn):\n",
    "    glove2word2vec(ext_model_fpn, word2vec_output_fpn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "King:  [-0.32307  -0.87616   0.21977   0.25268   0.22976   0.7388   -0.37954\n",
      " -0.35307  -0.84369  -1.1113   -0.30266   0.33178  -0.25113   0.30448\n",
      " -0.077491 -0.89815   0.092496 -1.1407   -0.58324   0.66869  -0.23122\n",
      " -0.95855   0.28262  -0.078848  0.75315   0.26584   0.3422   -0.33949\n",
      "  0.95608   0.065641  0.45747   0.39835   0.57965   0.39267  -0.21851\n",
      "  0.58795  -0.55999   0.63368  -0.043983 -0.68731  -0.37841   0.38026\n",
      "  0.61641  -0.88269  -0.12346  -0.37928  -0.38318   0.23868   0.6685\n",
      " -0.43321  -0.11065   0.081723  1.1569    0.78958  -0.21223  -2.3211\n",
      " -0.67806   0.44561   0.65707   0.1045    0.46217   0.19912   0.25802\n",
      "  0.057194  0.53443  -0.43133  -0.34311   0.59789  -0.58417   0.068995\n",
      "  0.23944  -0.85181   0.30379  -0.34177  -0.25746  -0.031101 -0.16285\n",
      "  0.45169  -0.91627   0.64521   0.73281  -0.22752   0.30226   0.044801\n",
      " -0.83741   0.55006  -0.52506  -1.7357    0.4751   -0.70487   0.056939\n",
      " -0.7132    0.089623  0.41394  -1.3363   -0.61915  -0.33089  -0.52881\n",
      "  0.16483  -0.98878 ]\n",
      "Most similar word to King + Woman:  [('queen', 0.7698541283607483)]\n"
     ]
    }
   ],
   "source": [
    "# load the converted word2vec model from glove\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format(word2vec_output_fpn, binary=False)\n",
    "\n",
    "# play around a bit\n",
    "# Show a word embedding\n",
    "print('King: ',model.get_vector('king'))\n",
    "result = model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
    "print('Most similar word to King + Woman: ', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7585 1897\n",
      "Made me go to Rehab. 11/10 EDIT: Suddenly, mods have stopped working for me. New Rating- 9/10(Until mods start working again).\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train),len(X_test))\n",
    "# print(list(X_train)[0])\n",
    "for review in X_train:\n",
    "    print(review)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a transformer class to engineer features\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class Word2VecVectorizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, model):\n",
    "        print('Loading in word vectors...')\n",
    "        self.word_vectors = model\n",
    "        print('Finished loading in word vectors')\n",
    "    def fit(self, data):\n",
    "        return self\n",
    "    def transform(self, data):\n",
    "        # determine the dimensionality of vectors\n",
    "        v = self.word_vectors.get_vector('king')\n",
    "        self.D = v.shape[0]\n",
    "\n",
    "        X = np.zeros((len(data), self.D))\n",
    "        n = 0\n",
    "        emptycount = 0\n",
    "        for sentence in data:\n",
    "            tokens = sentence.split()\n",
    "            vecs = []\n",
    "            m = 0\n",
    "            for word in tokens:\n",
    "                try:\n",
    "                    # throws KeyError if word not found\n",
    "                    vec = self.word_vectors.get_vector(word)\n",
    "                    vecs.append(vec)\n",
    "                    m += 1\n",
    "                except KeyError:\n",
    "                    pass\n",
    "            if len(vecs) > 0:\n",
    "                vecs = np.array(vecs)\n",
    "                X[n] = vecs.mean(axis=0)\n",
    "            else:\n",
    "                emptycount += 1\n",
    "            n += 1\n",
    "        print('Numer of samples with no words found: %s / %s' % (emptycount, len(data)))\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading in word vectors...\n",
      "Finished loading in word vectors\n",
      "Numer of samples with no words found: 163 / 7585\n",
      "Numer of samples with no words found: 35 / 1897\n"
     ]
    }
   ],
   "source": [
    "# Set a word vectorizer\n",
    "vectorizer = Word2VecVectorizer(model)\n",
    "# Get the review embeddings for the train dataset\n",
    "X_train_vec = vectorizer.fit_transform(list(X_train))\n",
    "X_test_vec = vectorizer.fit_transform(list(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.42893025e-02,  1.17876463e-01, -4.75554653e-02, ...,\n",
       "         1.10005174e-04,  4.93973553e-01,  7.54447728e-02],\n",
       "       [-1.65471748e-01,  1.68644950e-01,  3.98994893e-01, ...,\n",
       "        -3.38691026e-01,  5.80376565e-01,  1.38214588e-01],\n",
       "       [ 1.51719004e-01,  3.09051007e-01,  3.89197022e-01, ...,\n",
       "         2.60505021e-01,  1.07397997e+00, -1.25084996e-01],\n",
       "       ...,\n",
       "       [ 5.81400059e-02,  2.15204343e-01,  3.31174999e-01, ...,\n",
       "        -5.41083395e-01,  2.93883353e-01,  1.83979988e-01],\n",
       "       [ 2.14966744e-01,  6.03670001e-01,  5.06349981e-01, ...,\n",
       "        -2.75959998e-01,  5.13832510e-01,  2.51909733e-01],\n",
       "       [-7.77086839e-02,  2.22858697e-01,  3.86564970e-01, ...,\n",
       "        -2.10458115e-01,  4.10717547e-01,  1.68332383e-01]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "regressor = RandomForestRegressor()\n",
    "regressor.fit(X_train_vec, y_train)\n",
    "y_test_pred = regressor.predict(X_test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$r^2$ 0.28341488436188\n",
      "mean absolute error 0.11015411066125753\n",
      "mean absolute percentage error 6496846362957.67\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_absolute_percentage_error\n",
    "y_train_pred = regressor.predict(X_train_vec)\n",
    "print('$r^2$', r2_score(y_test, y_test_pred))\n",
    "print('mean absolute error', mean_absolute_error(y_test, y_test_pred))\n",
    "print('mean absolute percentage error', mean_absolute_percentage_error(y_test, y_test_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(729      0.666667\n",
       " 314      0.744186\n",
       " 1469     0.666667\n",
       " 12360    0.380000\n",
       " 132      0.968750\n",
       "            ...   \n",
       " 1003     0.676471\n",
       " 1575     0.571429\n",
       " 266      0.818182\n",
       " 103      0.819672\n",
       " 6081     0.861623\n",
       " Name: found_helpful_percentage, Length: 1897, dtype: float64,\n",
       " array([0.67566628, 0.7919915 , 0.65222544, ..., 0.71404562, 0.74579134,\n",
       "        0.85886407]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ca3056f1dc640b1c0f10c3ebc68c4b2c9a1c1009be407d94783946fd02c1914e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('tf-gpu': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
